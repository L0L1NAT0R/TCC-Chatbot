from flask import Flask, request, jsonify
from flask_cors import CORS
from openai import OpenAI
import os
import json
from dotenv import load_dotenv
import re
from pythainlp.tokenize import word_tokenize
from difflib import SequenceMatcher

load_dotenv()

app = Flask(__name__)
CORS(app)

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Load data
with open("brochures.json", encoding="utf-8") as f1:
    brochures = json.load(f1)

with open("articles.json", encoding="utf-8") as f2:
    articles = json.load(f2)

with open("about.json", encoding="utf-8") as f3:
    abouts = json.load(f3)

# Normalize text
def normalize(text):
    text = text.lower().strip()
    text = re.sub(r"[‚Äú‚Äù\"\'‚Äò‚Äô.,!?()\-\‚Äì‚Äî:;]", "", text)
    text = re.sub(r"\s+", " ", text)
    return text

# Matching logic
def filter_documents(user_msg, docs, max_docs=30, skip_title_filter=False):
    user_msg_clean = normalize(user_msg)
    user_tokens = word_tokenize(user_msg_clean, engine="newmm")

    filler_words = {
        "‡∏ó‡∏µ‡πà", "‡∏Ç‡∏≠‡∏á", "‡∏Å‡∏±‡∏ö", "‡πÇ‡∏î‡∏¢", "‡πÄ‡∏û‡∏∑‡πà‡∏≠", "‡∏ã‡∏∂‡πà‡∏á", "‡πÄ‡∏õ‡πá‡∏ô", "‡πÉ‡∏´‡πâ", "‡πÅ‡∏•‡πâ‡∏ß", "‡∏à‡∏∞",
        "‡∏Å‡πá", "‡∏¢‡∏±‡∏á", "‡πÅ‡∏•‡∏∞", "‡∏´‡∏£‡∏∑‡∏≠", "‡πÅ‡∏ï‡πà", "‡πÄ‡∏û‡∏£‡∏≤‡∏∞", "‡∏à‡∏∂‡∏á", "‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô", "‡πÅ‡∏°‡πâ", "‡∏´‡∏≤‡∏Å",
        "‡πÄ‡∏°‡∏∑‡πà‡∏≠", "‡∏à‡∏ô", "‡∏ï‡∏≤‡∏°", "‡∏Ç‡∏ì‡∏∞", "‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å", "‡∏ó‡∏≥", "‡∏°‡∏µ", "‡∏≠‡∏¢‡∏π‡πà", "‡πÑ‡∏õ", "‡∏°‡∏≤",
        "‡πÉ‡∏ä‡πâ", "‡∏ä‡πà‡∏ß‡∏¢", "‡∏ö‡∏≠‡∏Å", "‡∏£‡∏π‡πâ", "‡∏Ñ‡∏ß‡∏£", "‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ", "‡∏ï‡πâ‡∏≠‡∏á", "‡πÑ‡∏î‡πâ", "‡πÑ‡∏°‡πà", "‡∏â‡∏±‡∏ô",
        "‡∏Ñ‡∏∏‡∏ì", "‡πÄ‡∏£‡∏≤", "‡πÄ‡∏Ç‡∏≤", "‡πÄ‡∏ò‡∏≠", "‡∏°‡∏±‡∏ô", "‡∏´‡∏ô‡πà‡∏≠‡∏¢", "‡∏ô‡∏∞", "‡∏Ñ‡πà‡∏∞", "‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏à‡πâ‡∏≤",
        "‡∏à‡πä‡∏∞", "‡∏´‡∏ô‡∏∞", "‡∏•‡πà‡∏∞", "‡πÄ‡∏≠‡∏á", "‡∏™‡∏¥‡πà‡∏á", "‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á", "‡∏≠‡∏¢‡πà‡∏≤‡∏á", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
        "‡∏Ñ‡∏≥", "‡πÅ‡∏ö‡∏ö", "‡∏ä‡∏ô‡∏¥‡∏î", "‡∏≠‡∏∞‡πÑ‡∏£", "‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£", "‡∏ó‡∏≥‡πÑ‡∏°", "‡πÑ‡∏´‡∏ô", "‡πÉ‡∏Ñ‡∏£", "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà", "‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô"
    }

    signal_words = [
        w for w in user_tokens if w.strip() and w not in filler_words and len(w.strip()) >= 2
    ]

    semantic_triggers = {
        "overview": [
            "‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£", "‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö", "‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà", "‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£", "‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏≠‡∏∞‡πÑ‡∏£", "overview", "‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà"
        ],
        "mission": [
            "‡∏û‡∏±‡∏ô‡∏ò‡∏Å‡∏¥‡∏à", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢", "‡∏ó‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£", "‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó", "‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à", "mission"
        ],
        "history": [
            "‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥", "‡∏Å‡πà‡∏≠‡∏ï‡∏±‡πâ‡∏á", "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà", "‡∏õ‡∏µ", "‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô", "history"
        ],
        "revenue": [
            "‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ", "‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô", "‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì", "‡πÄ‡∏á‡∏¥‡∏ô", "income", "revenue", "‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°", "‡πÄ‡∏á‡∏¥‡∏ô‡∏≠‡∏∏‡∏î‡∏´‡∏ô‡∏∏‡∏ô"
        ],
        "contact": [
            "‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠", "‡πÄ‡∏ö‡∏≠‡∏£‡πå", "‡∏≠‡∏µ‡πÄ‡∏°‡∏•", "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà", "‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô", "contact", "phone", "address"
        ],
        "vision": [
            "‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î", "‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï", "‡πÄ‡∏´‡πá‡∏ô‡∏†‡∏≤‡∏û", "‡∏°‡∏∏‡πà‡∏á‡∏´‡∏ß‡∏±‡∏á", "vision"
        ],
        "strategy": [
            "‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå", "‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå", "‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô", "‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô", "strategy", "‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß", "‡πÅ‡∏ú‡∏ô 5 ‡∏õ‡∏µ"
        ]
    }


    prompt_boost_category = None
    for section, keywords in semantic_triggers.items():
        for kw in keywords:
            if kw in user_msg_clean or any(kw in token for token in user_tokens):
                prompt_boost_category = section
                break
        if prompt_boost_category:
            break

    scored = []

    for doc in docs:
        title = doc.get("title", "")
        content = doc.get("content", "")
        title_norm = normalize(title)
        content_norm = normalize(content)

        title_tokens = word_tokenize(title_norm, engine="newmm")
        content_tokens = word_tokenize(content_norm, engine="newmm") if content else []

        if not skip_title_filter and not any(word in title_tokens for word in signal_words):
            continue

        score = 0

        if prompt_boost_category and normalize(title) == prompt_boost_category:
            score += 5


        title_score = sum(
            2 if word == token else 1
            for word in signal_words
            for token in title_tokens
            if word in token or token in word
        )
        score += 2 * title_score

        content_score = sum(
            1
            for word in signal_words
            for token in content_tokens[:30]
            if word in token or token in word
        )
        score += min(content_score, 5)

        if skip_title_filter and content:
            similarity = SequenceMatcher(None, user_msg_clean, content_norm).ratio()
            if similarity >= 0.5:
                score += 3

        # ‚úÖ Hashtag match boost ‚Äî apply only if present
        # ‚úÖ Hashtag match boost ‚Äî apply only if present
        hashtags = doc.get("hashtags", [])
        hashtag_score = 0
        for tag in hashtags:
            tag_norm = normalize(tag)

            # Check direct inclusion (cleaned)
            if tag_norm in user_msg_clean:
                print(f"üéØ Exact hashtag matched: {tag_norm}")
                hashtag_score += 12
                continue

            # Check token overlap if not full phrase match
            tag_tokens = [tok for tok in word_tokenize(tag_norm, engine="newmm") if tok.strip()]
            match_count = sum(1 for tok in tag_tokens if any(tok in utok or utok in tok for utok in user_tokens))

            if match_count == len(tag_tokens):
                print(f"‚úÖ Full token overlap with hashtag: {tag_norm}")
                hashtag_score += 10
            elif match_count >= 1:
                print(f"‚ûï Partial token overlap with hashtag: {tag_norm}")
                hashtag_score += 5

        score += hashtag_score


        if score >= 3 or skip_title_filter:
            scored.append((score, doc))

    scored.sort(key=lambda x: x[0], reverse=True)

    print("‚úÖ MATCHED TITLES:")
    for score, doc in scored[:max_docs]:
        print(f"- ({score}) {doc.get('title', '‚ùå no title')}")

    return scored[:max_docs]

@app.route("/ask", methods=["POST"])
def ask():
    user_msg = request.json["message"]
    user_msg_clean = normalize(user_msg)  # move this BEFORE tokenization
    user_tokens = [tok for tok in word_tokenize(user_msg_clean, engine="newmm") if tok.strip()]

    about_keywords = [
        "‡∏™‡∏†‡∏≤‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ", "tcc", "‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£", "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó", "‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó", "‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà", "‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥", "‡∏Å‡πà‡∏≠‡∏ï‡∏±‡πâ‡∏á",
        "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢", "‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠", "‡πÄ‡∏ö‡∏≠‡∏£‡πå", "‡∏≠‡∏µ‡πÄ‡∏°‡∏•", "‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô", "‡πÄ‡∏•‡∏Ç‡∏≤‡∏ò‡∏¥‡∏Å‡∏≤‡∏£", "‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏ç‡πà"
    ]

    matched_about = [
        kw for kw in about_keywords
        if kw in user_msg_clean or any(kw in token for token in user_tokens)
    ]

    fuzzy_match = any(
        SequenceMatcher(None, kw, user_msg_clean).ratio() >= 0.85
        for kw in about_keywords
    )

    is_about_company = len(matched_about) > 0 or fuzzy_match

    print("üîç TOKENS:", user_tokens)
    print("üè¢ Matched about-org words:", matched_about)
    print("üß† Fuzzy match:", fuzzy_match)
    print("üè¢ Is about company:", is_about_company)

    if is_about_company:
        docs = abouts
    else:
        docs = brochures + articles

    filtered_docs = filter_documents(user_msg, docs, max_docs=3, skip_title_filter=is_about_company)

    if not filtered_docs:
        reply = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏Ñ‡πà‡∏∞"
    else:
        if is_about_company:
            top = filtered_docs[0] if filtered_docs else None
            if top and top[0] >= 3:
                score, doc = top
                title = doc.get("title", "")
                content = doc.get("content", "")
                reply = f"<strong>{title}</strong><br>{content[:800]}..."
            else:
                reply = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏Ñ‡πà‡∏∞"
        else:
            reply = ""
            for score, doc in filtered_docs[:3]:
                title = doc.get("title", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠")
                url = doc.get("url", "#")
                reply += f'- <a href="{url}" target="_blank">{title}</a><br>'
            if not reply:
                reply = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏Ñ‡πà‡∏∞"

    return jsonify({"reply": reply})

if __name__ == "__main__":
    print("‚úÖ Server running at http://localhost:5000")
    app.run(debug=True)
    