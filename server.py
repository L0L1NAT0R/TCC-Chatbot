from flask import Flask, request, jsonify
from flask_cors import CORS
from openai import OpenAI
import os
import json
from dotenv import load_dotenv
import re
from pythainlp.tokenize import word_tokenize
from difflib import SequenceMatcher

load_dotenv()

app = Flask(__name__)
CORS(app)

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Load data
with open("brochures.json", encoding="utf-8") as f1:
    brochures = json.load(f1)

with open("articles.json", encoding="utf-8") as f2:
    articles = json.load(f2)

with open("about.json", encoding="utf-8") as f3:
    abouts = json.load(f3)

# Normalize text
def normalize(text):
    text = text.lower().strip()
    text = re.sub(r"[‚Äú‚Äù\"\'‚Äò‚Äô.,!?()\-\‚Äì‚Äî:;]", "", text)
    text = re.sub(r"\s+", " ", text)
    return text

# Matching logic
def filter_documents(user_msg, docs, max_docs=30, skip_title_filter=False):
    user_msg_clean = normalize(user_msg)
    user_tokens = word_tokenize(user_msg_clean, engine="newmm")

    filler_words = {
        "‡∏ó‡∏µ‡πà", "‡∏Ç‡∏≠‡∏á", "‡∏Å‡∏±‡∏ö", "‡πÇ‡∏î‡∏¢", "‡πÄ‡∏û‡∏∑‡πà‡∏≠", "‡∏ã‡∏∂‡πà‡∏á", "‡πÄ‡∏õ‡πá‡∏ô", "‡πÉ‡∏´‡πâ", "‡πÅ‡∏•‡πâ‡∏ß", "‡∏à‡∏∞",
        "‡∏Å‡πá", "‡∏¢‡∏±‡∏á", "‡πÅ‡∏•‡∏∞", "‡∏´‡∏£‡∏∑‡∏≠", "‡πÅ‡∏ï‡πà", "‡πÄ‡∏û‡∏£‡∏≤‡∏∞", "‡∏à‡∏∂‡∏á", "‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô", "‡πÅ‡∏°‡πâ", "‡∏´‡∏≤‡∏Å",
        "‡πÄ‡∏°‡∏∑‡πà‡∏≠", "‡∏à‡∏ô", "‡∏ï‡∏≤‡∏°", "‡∏Ç‡∏ì‡∏∞", "‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å", "‡∏ó‡∏≥", "‡∏°‡∏µ", "‡∏≠‡∏¢‡∏π‡πà", "‡πÑ‡∏õ", "‡∏°‡∏≤",
        "‡πÉ‡∏ä‡πâ", "‡∏ä‡πà‡∏ß‡∏¢", "‡∏ö‡∏≠‡∏Å", "‡∏£‡∏π‡πâ", "‡∏Ñ‡∏ß‡∏£", "‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ", "‡∏ï‡πâ‡∏≠‡∏á", "‡πÑ‡∏î‡πâ", "‡πÑ‡∏°‡πà", "‡∏â‡∏±‡∏ô",
        "‡∏Ñ‡∏∏‡∏ì", "‡πÄ‡∏£‡∏≤", "‡πÄ‡∏Ç‡∏≤", "‡πÄ‡∏ò‡∏≠", "‡∏°‡∏±‡∏ô", "‡∏´‡∏ô‡πà‡∏≠‡∏¢", "‡∏ô‡∏∞", "‡∏Ñ‡πà‡∏∞", "‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏à‡πâ‡∏≤",
        "‡∏à‡πä‡∏∞", "‡∏´‡∏ô‡∏∞", "‡∏•‡πà‡∏∞", "‡πÄ‡∏≠‡∏á", "‡∏™‡∏¥‡πà‡∏á", "‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á", "‡∏≠‡∏¢‡πà‡∏≤‡∏á", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
        "‡∏Ñ‡∏≥", "‡πÅ‡∏ö‡∏ö", "‡∏ä‡∏ô‡∏¥‡∏î", "‡∏≠‡∏∞‡πÑ‡∏£", "‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£", "‡∏ó‡∏≥‡πÑ‡∏°", "‡πÑ‡∏´‡∏ô", "‡πÉ‡∏Ñ‡∏£", "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà", "‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô", "‡∏Å‡∏≤‡∏£"
    }

    signal_words = [
        w for w in user_tokens if w.strip() and w not in filler_words and len(w.strip()) >= 2
    ]

    semantic_triggers = {
        "overview": [
            "‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö", "‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà", "‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£", "‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏≠‡∏∞‡πÑ‡∏£", "overview", "‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà",
            "‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó", "‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏ô‡∏µ‡πâ", "tcc ‡∏Ñ‡∏∑‡∏≠", "tcc ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£", "‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö tcc", "tcc ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£"
        ],
        "mission": [
            "‡∏û‡∏±‡∏ô‡∏ò‡∏Å‡∏¥‡∏à", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢", "‡∏ó‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£", "‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó", "‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à", "mission"
        ],
        "history": [
            "‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥", "‡∏Å‡πà‡∏≠‡∏ï‡∏±‡πâ‡∏á", "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà", "‡∏õ‡∏µ", "‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô", "history"
        ],
        "revenue": [
            "‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ", "‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô", "‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì", "‡πÄ‡∏á‡∏¥‡∏ô", "income", "revenue", "‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°", "‡πÄ‡∏á‡∏¥‡∏ô‡∏≠‡∏∏‡∏î‡∏´‡∏ô‡∏∏‡∏ô"
        ],
        "contact": [
            "‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠", "‡πÄ‡∏ö‡∏≠‡∏£‡πå", "‡∏≠‡∏µ‡πÄ‡∏°‡∏•", "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà", "‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô", "contact", "phone", "address"
        ],
        "vision": [
            "‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î", "‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï", "‡πÄ‡∏´‡πá‡∏ô‡∏†‡∏≤‡∏û", "‡∏°‡∏∏‡πà‡∏á‡∏´‡∏ß‡∏±‡∏á", "vision", "‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß"
        ],
        "strategy": [
            "‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå", "‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå", "‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô", "‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô", "strategy", "‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß", "‡πÅ‡∏ú‡∏ô 5 ‡∏õ‡∏µ"
        ]
    }

    # Keywords describing consumer protection problems
    problem_keyword_boosts = {
        "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô": 3,
        "‡∏£‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô": 3,
        "‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢": 3,
        "‡∏´‡∏•‡∏≠‡∏Å": 3,
        "‡πÇ‡∏Å‡∏á": 3,
        "‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤": 3,
        "‡∏´‡∏•‡∏≠‡∏Å‡∏•‡∏ß‡∏á": 3,
        "‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏õ‡∏Å": 3,
        "‡∏ú‡∏¥‡∏î‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢": 3,
        "‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏á‡∏¥‡∏ô": 3,
        "‡∏Ç‡∏≠‡πÄ‡∏á‡∏¥‡∏ô‡∏Ñ‡∏∑‡∏ô": 3,
        "‡πÅ‡∏Æ‡∏Å": 3,
        "‡πÄ‡∏á‡∏¥‡∏ô‡∏´‡∏≤‡∏¢": 3,
        "‡∏ó‡∏∏‡∏à‡∏£‡∏¥‡∏ï": 3,
        "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö": 3,
        "‡∏™‡∏±‡∏ç‡∏ç‡∏≤": 3,
        "‡∏ú‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏±‡πà‡∏á": 3,
        "‡∏ä‡∏≥‡∏£‡∏∏‡∏î": 3,
        "‡πÄ‡∏Ñ‡∏•‡∏°‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô": 3
    }



    prompt_boost_category = None
    for section, keywords in semantic_triggers.items():
        for kw in keywords:
            if kw in user_msg_clean or any(kw in token for token in user_tokens):
                prompt_boost_category = section
                break
        if prompt_boost_category:
            break

    scored = []

    for doc in docs:
        title = doc.get("title", "")
        content = doc.get("content", "")
        title_norm = normalize(title)
        content_norm = normalize(content)

        title_tokens = word_tokenize(title_norm, engine="newmm")
        content_tokens = word_tokenize(content_norm, engine="newmm") if content else []

        if not skip_title_filter and not (
            any(word in title_tokens for word in signal_words) or
            prompt_boost_category == title_norm
        ):
            continue

        score = 0

        if prompt_boost_category and normalize(title) == prompt_boost_category:
            score += 5


        title_score = sum(
            2 if word == token else 1
            for word in signal_words
            for token in title_tokens
            if word in token or token in word
        )
        score += 2 * title_score

        content_score = sum(
            1
            for word in signal_words
            for token in content_tokens[:30]
            if word in token or token in word
        )
        score += min(content_score, 5)

        if skip_title_filter and content:
            similarity = SequenceMatcher(None, user_msg_clean, content_norm).ratio()
            if similarity >= 0.5:
                score += 3

        # ‚úÖ Hashtag match boost ‚Äî apply only if present
        # ‚úÖ Hashtag match boost ‚Äî apply only if present
        hashtags = doc.get("hashtags", [])
        hashtag_score = 0
        for tag in hashtags:
            tag_norm = normalize(tag)

            # Check direct inclusion (cleaned)
            if tag_norm in user_msg_clean:
                print(f"üéØ Exact hashtag matched: {tag_norm}")
                hashtag_score += 12
                continue

            # Check token overlap if not full phrase match
            tag_tokens = [tok for tok in word_tokenize(tag_norm, engine="newmm") if tok.strip()]
            match_count = sum(1 for tok in tag_tokens if any(tok in utok or utok in tok for utok in user_tokens))

            if match_count == len(tag_tokens):
                print(f"‚úÖ Full token overlap with hashtag: {tag_norm}")
                hashtag_score += 10
            elif match_count >= 1:
                print(f"‚ûï Partial token overlap with hashtag: {tag_norm}")
                hashtag_score += 5

        score += hashtag_score

                # Boost for problem-related keywords in user message
        for word in signal_words:
            boost = problem_keyword_boosts.get(word, 0)
            if(boost > 0):
                print(f"üí• Boosted for keyword: {word} (+{boost})")
                score

        if score >= 3 or skip_title_filter:
            scored.append((score, doc))

    scored.sort(key=lambda x: x[0], reverse=True)

    print("‚úÖ MATCHED TITLES:")
    for score, doc in scored[:max_docs]:
        print(f"- ({score}) {doc.get('title', '‚ùå no title')}")

    return scored[:max_docs]

@app.route("/ask", methods=["POST"])
def ask():
    user_msg = request.json["message"]
    user_msg_clean = normalize(user_msg)  # move this BEFORE tokenization
    user_tokens = [tok for tok in word_tokenize(user_msg_clean, engine="newmm") if tok.strip()]

    # ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏ô ask()
    smalltalk_keywords = [
        "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "‡∏Ñ‡∏∏‡∏ì‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£", "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏Ñ‡∏£", "‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á", "‡∏™‡∏ö‡∏≤‡∏¢‡∏î‡∏µ‡πÑ‡∏´‡∏°", "‡∏´‡∏ß‡∏±‡∏î‡∏î‡∏µ", "‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢", "‡∏™‡∏ö‡∏≤‡∏¢‡∏î‡∏µ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤", "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏ö‡πâ‡∏≤‡∏á", "‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏á‡∏ö‡πâ‡∏≤‡∏á", "‡πÄ‡∏à‡∏≠‡∏Å‡∏±‡∏ô‡∏≠‡∏µ‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡∏ô‡∏∞", "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏à‡∏≠‡∏Å‡∏±‡∏ô‡∏ô‡∏≤‡∏ô‡πÄ‡∏•‡∏¢", "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å", "‡∏Ñ‡∏∏‡∏ì‡∏™‡∏ö‡∏≤‡∏¢‡∏î‡∏µ‡πÑ‡∏´‡∏°", "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡πÇ‡∏≠‡πÄ‡∏Ñ‡πÑ‡∏´‡∏°",
        "‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏î‡∏µ‡πÑ‡∏´‡∏°", "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢‡πÑ‡∏´‡∏°", "‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡∏≠‡∏¢‡∏π‡πà", "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏∏‡πà‡∏á‡πÑ‡∏´‡∏°", "‡πÑ‡∏î‡πâ‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô‡πÑ‡∏´‡∏°", "‡∏Å‡∏¥‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á"
    ]

    if any(kw in user_msg_clean for kw in smalltalk_keywords):
        messages = [
            {"role": "system", "content": (
                "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£\n"
                "‡∏ñ‡πâ‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏Ñ‡∏∏‡∏ì ‡πÄ‡∏ä‡πà‡∏ô ‡∏ä‡∏∑‡πà‡∏≠ ‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢\n"
                "‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏≠‡∏∑‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô\n"
            )},
            {"role": "user", "content": user_msg}
        ]

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages
        )
        reply = response.choices[0].message.content
        return jsonify({"reply": reply})

        
    #NORMAL RESPONSES
    about_keywords = [
        "‡∏™‡∏†‡∏≤‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ", "tcc", "‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£", "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó", "‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó", "‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà", "‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥", "‡∏Å‡πà‡∏≠‡∏ï‡∏±‡πâ‡∏á",
        "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢", "‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠", "‡πÄ‡∏ö‡∏≠‡∏£‡πå", "‡∏≠‡∏µ‡πÄ‡∏°‡∏•", "‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô", "‡πÄ‡∏•‡∏Ç‡∏≤‡∏ò‡∏¥‡∏Å‡∏≤‡∏£", "‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏ç‡πà"
    ]

    matched_about = [
        kw for kw in about_keywords
        if kw in user_msg_clean or any(kw in token for token in user_tokens)
    ]

    fuzzy_match = any(
        SequenceMatcher(None, kw, user_msg_clean).ratio() >= 0.85
        for kw in about_keywords
    )

    is_about_company = len(matched_about) > 0 or fuzzy_match

    print("üîç TOKENS:", user_tokens)
    print("üè¢ Matched about-org words:", matched_about)
    print("üß† Fuzzy match:", fuzzy_match)
    print("üè¢ Is about company:", is_about_company)

    if is_about_company:
        docs = abouts
    else:
        docs = brochures + articles

    filtered_docs = filter_documents(user_msg, docs, max_docs=3, skip_title_filter=is_about_company)
    print("üìã Filtered docs (score, title):")
    for score, doc in filtered_docs:
        print(f"  - ({score}) {doc.get('title')}")

    if not filtered_docs:
        reply = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏Ñ‡πà‡∏∞"
    else:
        if is_about_company:
            prompt = (
                "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n"
                "‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡πÄ‡∏ä‡πà‡∏ô ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥ ‡∏û‡∏±‡∏ô‡∏ò‡∏Å‡∏¥‡∏à ‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå ‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠\n"
                "‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ\n"
                "‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÅ‡∏£‡∏Å‡∏Ñ‡∏∑‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö ‡πÅ‡∏°‡πâ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° 100%\n"
                "‡πÅ‡∏™‡∏î‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏¢‡πà‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n"
                "‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà\n\n"
            )

            for score, doc in filtered_docs[:3]:
                title = doc.get("title", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠")
                content = doc.get("content", "")
                prompt += f"- {title}\n{content.strip()[:500]}...\n\n"

            messages = [
                {"role": "system", "content": prompt},
                {"role": "user", "content": user_msg}
            ]

            response = client.chat.completions.create(
                model="gpt-4o",
                messages=messages
            )
            reply = response.choices[0].message.content

        else:
            reply = ""
            for score, doc in filtered_docs[:3]:
                title = doc.get("title", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠")
                url = doc.get("url", "#")
                reply += f'- <a href="{url}" target="_blank">{title}</a><br>'
            if not reply:
                reply = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏Ñ‡πà‡∏∞"

    return jsonify({"reply": reply})

if __name__ == "__main__":
    print("‚úÖ Server running at http://localhost:5000")
    app.run(debug=True)