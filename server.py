from flask import Flask, request, jsonify #use flask web server
from flask_cors import CORS #allow frontend the input to talk with the server so the server can give output
from openai import OpenAI #use gpt 3.5 to answer question
import os
import json
import re
from dotenv import load_dotenv #load API key in GPT
from embedding_utils import get_embedding, cosine_similarity
import numpy as np #for cosine similarity work with vectors

load_dotenv()
app = Flask(__name__)
CORS(app)

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Load data from all json files that were compiled using the scrape scripts 
with open("brochures.json", encoding="utf-8") as f1:
    brochures = json.load(f1)
    for b in brochures:
        b["source"] = "brochure"

with open("articles.json", encoding="utf-8") as f2:
    articles = json.load(f2)
    for a in articles:
        a["source"] = "article"

with open("about.json", encoding="utf-8") as f3:
    abouts = json.load(f3)
    for ab in abouts:
        ab["source"] = "about"

# Embed 'abouts' live and ensure embeddings are list-type (not np.array)
for doc in abouts:
    content = doc.get("title", "") + " " + doc.get("content", "")
    emb = get_embedding(content[:1000])
    doc["embedding"] = emb.tolist() if hasattr(emb, "tolist") else emb

with open("videos.json", encoding = "utf-8") as f4:
    videos = json.load(f4)
    for v in videos:
        v["source"] = "video"

all_docs = brochures + articles + videos



# Embed all documents into vectors used to compare similarity
# Load precomputed embeddings
print("üì• Loading embedded documents...")
with open("embedded_docs.json", encoding="utf-8") as f:
    all_docs = json.load(f)
print(f"‚úÖ Loaded {len(all_docs)} embedded documents")

#basically removes common punctuations in the string to purely evaluate word meaning
def normalize(text):
    text = text.lower().strip()
    text = re.sub(r"[‚Äú‚Äù\"\'‚Äò‚Äô.,!?()\-\‚Äì‚Äî:;]", "", text)
    text = re.sub(r"\s+", " ", text)
    return text
#use gpt 4o to perform classification and return a binary choice of reteurn link or user based on the links
def classify_user_intent(user_input):
    messages = [
        {"role": "system", "content": (
            "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ:\n"
            "- ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö: ‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£\n"
            "- ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏ß ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö: ‡∏•‡∏¥‡∏á‡∏Å‡πå\n"
            "‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ '‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£' ‡∏´‡∏£‡∏∑‡∏≠ '‡∏•‡∏¥‡∏á‡∏Å‡πå' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"
        )},
        {"role": "user", "content": user_input}
    ]
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages
    )
    return response.choices[0].message.content.strip()


#this function looks at cosine similarity of embedding and picks the top 30 most similar in meaning and etc.
def get_top_documents_by_similarity(user_input, docs, top_k=30):
    user_embedding = get_embedding(user_input)
    scored = [(cosine_similarity(user_embedding, np.array(doc["embedding"])), doc) for doc in docs]
    scored.sort(reverse=True, key=lambda x: x[0])
    return [doc for _, doc in scored[:top_k]]
#have the model rerank the top 30 and select the top 3 most relevant
def gpt_rerank_documents(user_input, docs):
    doc_list_text = ""
    for i, doc in enumerate(docs):
        title = doc.get("title", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠")
        content = doc.get("content", "") or doc.get("description", "")
        doc_list_text += f"{i+1}. {title.strip()} ‚Äî {content.strip()[:150]}...\n"

    system_prompt = (
        "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ "
        "‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á\n\n"
        f"{doc_list_text}\n\n"
        "‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î "
        "‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≠‡∏°‡∏°‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô 1, 4, 8 (‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô)"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages
    )

    answer = response.choices[0].message.content.strip()
    print("üìå GPT selected indexes:", answer)

    indexes = [int(i) - 1 for i in re.findall(r"\d+", answer)]
    return [docs[i] for i in indexes if 0 <= i < len(docs)]


@app.route("/ask", methods=["POST"])
def ask():
    user_msg = request.json["message"]
    intent = classify_user_intent(user_msg)
    print("üìå Intent:", intent)
#perform this if asked about the company
    if intent == "‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£":
        docs = get_top_documents_by_similarity(user_msg, abouts, top_k=10)
        top_docs = gpt_rerank_documents(user_msg, docs)
        prompt = "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n\n"
        for doc in top_docs:
            prompt += f"- {doc['title']}: {doc.get('content', '')[:500]}...\n"
        messages = [{"role": "system", "content": prompt}, {"role": "user", "content": user_msg}]
        response = client.chat.completions.create(model="gpt-4o", messages=messages)
        reply = response.choices[0].message.content
    else:
        docs = get_top_documents_by_similarity(user_msg, all_docs, top_k=30)
        top_docs = gpt_rerank_documents(user_msg, docs)
        if not top_docs:
            reply = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏Ñ‡πà‡∏∞"
        else:
            reply = ""
            for doc in top_docs:
                title = doc.get("title", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠")
                url = doc.get("url", "#")
                source = doc.get("source", "")

                if source == "brochure":
                    label = "üìä ‡∏≠‡∏¥‡∏ô‡πÇ‡∏ü‡∏Å‡∏£‡∏≤‡∏ü‡∏¥‡∏Å‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå"
                    reply += f'{label}<br>- <a href="{url}" target="_blank">{title}</a><br><br>'

                elif source == "article":
                    label = "üì∞ ‡∏Ç‡πà‡∏≤‡∏ß/‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå"
                    reply += f'{label}<br>- <a href="{url}" target="_blank">{title}</a><br><br>'

                elif source == "video":
                    label = "üé• ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏à‡∏≤‡∏Å YouTube ‡∏ä‡πà‡∏≠‡∏á TCC"
                    video_id = url.split("v=")[-1] if "v=" in url else ""
                    thumbnail_url = f"https://img.youtube.com/vi/{video_id}/hqdefault.jpg"
                    reply += (
                        f'{label}<br>'
                        f'<a href="{url}" target="_blank">'
                        f'<img src="{thumbnail_url}" alt="{title}" style="width:100%;max-width:320px;border-radius:10px;margin-bottom:4px;"><br>'
                        f'{title}</a><br><br>'
                    )

                else:
                    label = "üìÅ ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏∑‡πà‡∏ô"
                    reply += f'{label}<br>- <a href="{url}" target="_blank">{title}</a><br><br>'



    return jsonify({"reply": reply})

if __name__ == "__main__":
    app.run(debug=True)