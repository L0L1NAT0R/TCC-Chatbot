from flask import Flask, request, jsonify
from flask_cors import CORS
from openai import OpenAI
import os
import json
from dotenv import load_dotenv
import re
from pythainlp.tokenize import word_tokenize
from difflib import SequenceMatcher

load_dotenv()

app = Flask(__name__)
CORS(app)

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Load data
with open("brochures.json", encoding="utf-8") as f1:
    brochures = json.load(f1)

with open("articles.json", encoding="utf-8") as f2:
    articles = json.load(f2)

with open("about.json", encoding="utf-8") as f3:
    abouts = json.load(f3)

# Normalize text
def normalize(text):
    text = text.lower().strip()
    text = re.sub(r"[‚Äú‚Äù\"\'‚Äò‚Äô.,!?()\-\‚Äì‚Äî:;]", "", text)
    text = re.sub(r"\s+", " ", text)
    return text

# Matching logic
def filter_documents(user_msg, docs, max_docs=30, skip_title_filter=False):
    user_msg_clean = normalize(user_msg)
    user_tokens = word_tokenize(user_msg_clean, engine="newmm")

    filler_words = {
        "‡∏ó‡∏µ‡πà", "‡∏Ç‡∏≠‡∏á", "‡∏Å‡∏±‡∏ö", "‡πÇ‡∏î‡∏¢", "‡πÄ‡∏û‡∏∑‡πà‡∏≠", "‡∏ã‡∏∂‡πà‡∏á", "‡πÄ‡∏õ‡πá‡∏ô", "‡πÉ‡∏´‡πâ", "‡πÅ‡∏•‡πâ‡∏ß", "‡∏à‡∏∞",
        "‡∏Å‡πá", "‡∏¢‡∏±‡∏á", "‡πÅ‡∏•‡∏∞", "‡∏´‡∏£‡∏∑‡∏≠", "‡πÅ‡∏ï‡πà", "‡πÄ‡∏û‡∏£‡∏≤‡∏∞", "‡∏à‡∏∂‡∏á", "‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô", "‡πÅ‡∏°‡πâ", "‡∏´‡∏≤‡∏Å",
        "‡πÄ‡∏°‡∏∑‡πà‡∏≠", "‡∏à‡∏ô", "‡∏ï‡∏≤‡∏°", "‡∏Ç‡∏ì‡∏∞", "‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å", "‡∏ó‡∏≥", "‡∏°‡∏µ", "‡∏≠‡∏¢‡∏π‡πà", "‡πÑ‡∏õ", "‡∏°‡∏≤",
        "‡πÉ‡∏ä‡πâ", "‡∏ä‡πà‡∏ß‡∏¢", "‡∏ö‡∏≠‡∏Å", "‡∏£‡∏π‡πâ", "‡∏Ñ‡∏ß‡∏£", "‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ", "‡∏ï‡πâ‡∏≠‡∏á", "‡πÑ‡∏î‡πâ", "‡πÑ‡∏°‡πà", "‡∏â‡∏±‡∏ô",
        "‡∏Ñ‡∏∏‡∏ì", "‡πÄ‡∏£‡∏≤", "‡πÄ‡∏Ç‡∏≤", "‡πÄ‡∏ò‡∏≠", "‡∏°‡∏±‡∏ô", "‡∏´‡∏ô‡πà‡∏≠‡∏¢", "‡∏ô‡∏∞", "‡∏Ñ‡πà‡∏∞", "‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏à‡πâ‡∏≤",
        "‡∏à‡πä‡∏∞", "‡∏´‡∏ô‡∏∞", "‡∏•‡πà‡∏∞", "‡πÄ‡∏≠‡∏á", "‡∏™‡∏¥‡πà‡∏á", "‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á", "‡∏≠‡∏¢‡πà‡∏≤‡∏á", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
        "‡∏Ñ‡∏≥", "‡πÅ‡∏ö‡∏ö", "‡∏ä‡∏ô‡∏¥‡∏î", "‡∏≠‡∏∞‡πÑ‡∏£", "‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£", "‡∏ó‡∏≥‡πÑ‡∏°", "‡πÑ‡∏´‡∏ô", "‡πÉ‡∏Ñ‡∏£", "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà", "‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô"
    }

    signal_words = [
        w for w in user_tokens if w.strip() and w not in filler_words and len(w.strip()) >= 2
    ]

    # üîç Define semantic trigger words
    semantic_triggers = {
        "overview": ["‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£", "‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö", "‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà", "‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£", "‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏≠‡∏∞‡πÑ‡∏£", "overview", "‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà"],
        "mission": ["‡∏û‡∏±‡∏ô‡∏ò‡∏Å‡∏¥‡∏à", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢", "‡∏ó‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£", "‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó", "‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à", "mission"],
        "history": ["‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥", "‡∏Å‡πà‡∏≠‡∏ï‡∏±‡πâ‡∏á", "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà", "‡∏õ‡∏µ", "‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô", "history"],
        "revenue": ["‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ", "‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô", "‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì", "‡πÄ‡∏á‡∏¥‡∏ô", "income", "revenue"],
        "contact": ["‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠", "‡πÄ‡∏ö‡∏≠‡∏£‡πå", "‡∏≠‡∏µ‡πÄ‡∏°‡∏•", "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà", "‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô", "contact", "phone", "address"]
    }

    # üîç Detect which category the prompt most likely refers to
    prompt_boost_category = None
    for section, keywords in semantic_triggers.items():
        for kw in keywords:
            if kw in user_msg_clean or any(kw in token for token in user_tokens):
                prompt_boost_category = section
                break
        if prompt_boost_category:
            break

    scored = []

    for doc in docs:
        title = doc.get("title", "")
        content = doc.get("content", "")
        title_norm = normalize(title)
        content_norm = normalize(content)

        title_tokens = word_tokenize(title_norm, engine="newmm")
        content_tokens = word_tokenize(content_norm, engine="newmm") if content else []

        if not skip_title_filter and not any(word in title_tokens for word in signal_words):
            continue

        score = 0

        # üéØ Intent alignment: boost if user's prompt and title agree
        if prompt_boost_category and prompt_boost_category in title_norm:
            score += 5

        # üî° Token title match
        title_score = sum(
            2 if word == token else 1
            for word in signal_words
            for token in title_tokens
            if word in token or token in word
        )
        score += 2 * title_score

        # üìÑ Content match (light)
        content_score = sum(
            1
            for word in signal_words
            for token in content_tokens[:30]
            if word in token or token in word
        )
        score += min(content_score, 5)

        # üß† Fuzzy match to content (for long answers)
        if skip_title_filter and content:
            similarity = SequenceMatcher(None, user_msg_clean, content_norm).ratio()
            if similarity >= 0.5:
                score += 3

        # ‚úÖ Include if it's clearly relevant or if we're in fallback mode
        if score >= 3 or skip_title_filter:
            scored.append((score, doc))

    scored.sort(key=lambda x: x[0], reverse=True)

    # Debug output
    print("‚úÖ MATCHED TITLES:")
    for score, doc in scored[:max_docs]:
        print(f"- ({score}) {doc.get('title', '‚ùå no title')}")

    return scored[:max_docs]  # returns list of (score, doc)


@app.route("/ask", methods=["POST"])
def ask():
    user_msg = request.json["message"]
    user_tokens = word_tokenize(user_msg.lower(), engine="newmm")

    about_keywords = [
        "tcc", "‡∏™‡∏†‡∏≤‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ", "‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö", "‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥", "‡∏ó‡∏µ‡πà‡∏°‡∏≤", "‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå",
        "‡∏û‡∏±‡∏ô‡∏ò‡∏Å‡∏¥‡∏à", "‡∏Å‡πà‡∏≠‡∏ï‡∏±‡πâ‡∏á", "‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£", "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó", "‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô", "‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢"
    ]

    matched_about = [kw for kw in about_keywords if kw in user_tokens]
    fuzzy_match = any(
        SequenceMatcher(None, kw, token).ratio() >= 0.8
        for kw in about_keywords
        for token in user_tokens
    )
    is_about_company = len(matched_about) > 0 or fuzzy_match

    print("üîç TOKENS:", user_tokens)
    print("üè¢ Matched about-org words:", matched_about)
    print("üß† Fuzzy match:", fuzzy_match)
    print("üè¢ Is about company:", is_about_company)

    if is_about_company:
        docs = abouts
    else:
        docs = brochures + articles

    filtered_docs = filter_documents(user_msg, docs, max_docs=3, skip_title_filter=is_about_company)

    if not filtered_docs:
        reply = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏Ñ‡πà‡∏∞"
    else:
        if is_about_company:
            top = filtered_docs[0] if filtered_docs else None
            if top and top[0] >= 3:
                score, doc = top
                title = doc.get("title", "")
                content = doc.get("content", "")
                reply = f"<strong>{title}</strong><br>{content[:800]}..."
            else:
                reply = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏Ñ‡πà‡∏∞"

        else:
            reply = ""
            for score, doc in filtered_docs[:3]:
                title = doc.get("title", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠")
                url = doc.get("url", "#")
                reply += f'- <a href="{url}" target="_blank">{title}</a><br>'
            if not reply:
                reply = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏Ñ‡πà‡∏∞"

    return jsonify({"reply": reply})

if __name__ == "__main__":
    print("‚úÖ Server running at http://localhost:5000")
    app.run(debug=True)
    